from copy import deepcopy


conversion_rate_phase1 = np.array([[0.91,0.93,0.79],
                                  [0.85,0.82,0.45], #2*price
                                  [0.48,0.61,0.31], #3*price
                                  [0.36,0.56,0.24], #4*price
                                  [0.08,0.19,0.13]  #5*price
                                                  ])
                                      #C1   C2   C3
conversion_rate_phase2 =  np.array([[0.93, 0.86, 0.78],  # 1*price
                                    [0.88, 0.77, 0.39],  # 2*price
                                    [0.71, 0.61, 0.28],  # 3*price
                                    [0.28, 0.44, 0.17],  # 4*price
                                    [0.13, 0.12, 0.10]   # 5*price
                                    ])

                                      #C1   C2   C3
conversion_rate_phase3 =  np.array([[0.96, 0.93, 0.83],  # 1*price
                                    [0.84, 0.89, 0.47],  # 2*price
                                    [0.48, 0.66, 0.33],  # 3*price
                                    [0.39, 0.52, 0.22],  # 4*price
                                    [0.15, 0.21, 0.14]   # 5*price
                                    ])

earnings_phase1 = np.zeros([5,3]) # conv_rate * margin
earnings_phase2 = np.zeros([5,3]) # conv_rate * margin
earnings_phase3 = np.zeros([5,3]) # conv_rate * margin

for row in range(5):
  earnings_phase1[row,:] = conversion_rate_phase1[row,:] * margins[row]
  earnings_phase2[row,:] = conversion_rate_phase2[row,:] * margins[row]
  earnings_phase3[row,:] = conversion_rate_phase3[row,:] * margins[row]

normEarnings_phase1 = earnings_phase1.copy()
normEarnings_phase1 = normEarnings_phase1 - np.min(normEarnings_phase1)
normEarnings_phase1 = normEarnings_phase1 / np.max(normEarnings_phase1)

normEarnings_phase2 = earnings_phase2.copy()
normEarnings_phase2 = normEarnings_phase2 - np.min(normEarnings_phase2)
normEarnings_phase2 = normEarnings_phase2 / np.max(normEarnings_phase2)

normEarnings_phase3 = earnings_phase3.copy()
normEarnings_phase3 = normEarnings_phase3 - np.min(normEarnings_phase3)
normEarnings_phase3 = normEarnings_phase3 / np.max(normEarnings_phase3)


env_array = []
T = 365
for c in classes:
  env_array.append(Non_Stationary_Environment(n_prices, np.array([normEarnings_phase1[:,c], normEarnings_phase2[:,c], normEarnings_phase3[:,c]]), c, T, 0))

#EXPERIMENT BEGIN


n_experiments = 100

M = 100 #number of steps to obtain reference point in change detection (for CUSUM)
eps = 0.1 #epsilon for deviation from reference point in change detection (for CUSUM)
h = np.log(T)*2 #threshold for change detection (for CUSUM)

ucb1_rewards_per_experiments = []
swucb_rewards_per_experiments = []
cusum_rewards_per_experiments = []

opt_index_phase1 = int(clairvoyant(classes,bids,prices, margins,conversion_rate_phase1,env_array)[0][0])
opt_phase1 = normEarnings_phase1[opt_index_phase1][0]
optimal_bid_index_phase1 = clairvoyant(classes,bids,prices, margins,conversion_rate_phase1,env_array)[1][0]
optimal_bid_phase1 = bids[int(optimal_bid_index_phase1)]

opt_index_phase2 = int(clairvoyant(classes,bids,prices, margins,conversion_rate_phase2,env_array)[0][0])
opt_phase2 = normEarnings_phase2[opt_index_phase2][0]
optimal_bid_index_phase2 = clairvoyant(classes,bids,prices, margins,conversion_rate_phase2,env_array)[1][0]
optimal_bid_phase2 = bids[int(optimal_bid_index_phase2)]

opt_index_phase3 = int(clairvoyant(classes,bids,prices, margins,conversion_rate_phase3,env_array)[0][0])
opt_phase3 = normEarnings_phase3[opt_index_phase3][0]
optimal_bid_index_phase3 = clairvoyant(classes,bids,prices, margins,conversion_rate_phase3,env_array)[1][0]
optimal_bid_phase3 = bids[int(optimal_bid_index_phase3)]


for e in tqdm(range(n_experiments)):
  env_swucb = deepcopy(env_array[0])
  env_cusum = deepcopy(env_array[0])
  env_ucb1 = deepcopy(env_array[0])

  swucb_learner = SWUCB_Learner(n_arms = n_prices, window_size = int(T/3))
  cusum_learner = CUSUM_UCB_Learner(n_arms = n_prices, M = M, eps = eps, h = h)
  ucb1_learner = UCB1_Learner(n_arms = n_prices)
  for t in range(0, T):

    pulled_arm = swucb_learner.pull_arm()
    reward = env_swucb.round(pulled_arm)
    swucb_learner.update(pulled_arm, reward)

    pulled_arm = cusum_learner.pull_arm()
    reward = env_cusum.round(pulled_arm)
    cusum_learner.update(pulled_arm, reward)

    pulled_arm = ucb1_learner.pull_arm()
    reward = env_ucb1.round(pulled_arm)
    ucb1_learner.update(pulled_arm, reward)

  swucb_rewards_per_experiments.append(swucb_learner.collected_rewards)
  cusum_rewards_per_experiments.append(cusum_learner.collected_rewards)
  ucb1_rewards_per_experiments.append(ucb1_learner.collected_rewards)

swucb_rewards_per_experiments = np.array(swucb_rewards_per_experiments)
cusum_rewards_per_experiments = np.array(cusum_rewards_per_experiments)
ucb1_rewards_per_experiments = np.array(ucb1_rewards_per_experiments)

fig, axs = plt.subplots(2,2,figsize=(14,7))

opt_phase1 = opt_phase1 * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
opt_phase2 = opt_phase2 * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
opt_phase3 = opt_phase3 * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)
opt = np.ones([T])
opt[:int(T/3)] = opt[:int(T/3)] * opt_phase1
opt[int(T/3):2*int(T/3)] = opt[int(T/3):2*int(T/3)]* opt_phase2
opt[2*int(T/3):] = opt[2*int(T/3):] * opt_phase3

swucb_rewards_per_experiments[:int(T/3)] = swucb_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
swucb_rewards_per_experiments[int(T/3):2*int(T/3)] = swucb_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
swucb_rewards_per_experiments[2*int(T/3):] = swucb_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)

cusum_rewards_per_experiments[:int(T/3)] = cusum_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
cusum_rewards_per_experiments[int(T/3):2*int(T/3)] = cusum_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
cusum_rewards_per_experiments[2*int(T/3):] = cusum_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)

ucb1_rewards_per_experiments[:int(T/3)] = ucb1_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
ucb1_rewards_per_experiments[int(T/3):2*int(T/3)] = ucb1_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
ucb1_rewards_per_experiments[2*int(T/3):] = ucb1_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)


axs[0][0].set_xlabel("t")
axs[0][0].set_ylabel("Regret")
axs[0][0].plot(np.cumsum(np.mean(swucb_rewards_per_experiments, axis = 0)), 'tab:blue')
axs[0][0].plot(np.cumsum(np.mean(cusum_rewards_per_experiments, axis = 0)), 'tab:cyan')
axs[0][0].plot(np.cumsum(np.mean(ucb1_rewards_per_experiments, axis = 0)), 'tab:red')

#We plot only the standard deviation of the reward beacuse the standard deviation of the regret is the same
axs[0][0].plot(np.cumsum(np.std(swucb_rewards_per_experiments, axis = 0)), 'tab:orange')
axs[0][0].plot(np.cumsum(np.std(cusum_rewards_per_experiments, axis = 0)), 'tab:purple')
axs[0][0].plot(np.cumsum(np.std(ucb1_rewards_per_experiments, axis = 0)), 'tab:green')

axs[0][0].plot(np.cumsum(np.mean(opt - swucb_rewards_per_experiments, axis = 0)), 'tab:olive')
axs[0][0].plot(np.cumsum(np.mean(opt - cusum_rewards_per_experiments, axis = 0)), 'tab:pink')
axs[0][0].plot(np.cumsum(np.mean(opt - ucb1_rewards_per_experiments, axis = 0)), 'tab:brown')

axs[0][0].legend(["Reward SWUCB","Reward CUSUM","Reward UCB1","Std SWUCB","Std CUSUM","Std UCB1","Regret SWUCB","Regret CUSUM","Regret UCB1"])
axs[0][0].set_title("Cumulative SWUCB vs CUSUM vs UCB1")


axs[0][1].set_xlabel("t")
axs[0][1].set_ylabel("Regret")
axs[0][1].plot(np.mean(swucb_rewards_per_experiments, axis = 0), 'r')
axs[0][1].plot(np.mean(cusum_rewards_per_experiments, axis = 0), 'm')
axs[0][1].plot(np.mean(ucb1_rewards_per_experiments, axis = 0), 'b')
axs[0][1].legend(["Reward SWUCB", "Reward CUSUM", "Reward UCB1"])
axs[0][1].set_title("Instantaneous Reward SWUCB vs CUSUM vs UCB1")

#We plot only the standard deviation of the reward beacuse the standard deviation of the regret is the same
axs[1][0].plot(np.std(swucb_rewards_per_experiments, axis = 0), 'b')
axs[1][0].plot(np.std(cusum_rewards_per_experiments, axis = 0), 'c')
axs[1][0].plot(np.std(ucb1_rewards_per_experiments, axis = 0), 'r')
axs[1][0].legend(["Std SWUCB","Std CUSUM","Std UCB1"])
axs[1][0].set_title("Instantaneous Std SWUCB vs CUSUM vs UCB1")

axs[1][1].plot(np.mean(opt - swucb_rewards_per_experiments, axis = 0), 'g')
axs[1][1].plot(np.mean(opt - cusum_rewards_per_experiments, axis = 0), 'y')
axs[1][1].plot(np.mean(opt - ucb1_rewards_per_experiments, axis = 0), 'k')
axs[1][1].legend(["Regret SWUCB","Regret CUSUM","Regret UCB1"])
axs[1][1].set_title("Instantaneous Regret SWUCB vs CUSUM vs UCB1")


plt.show()

##SENSITIVTY ANALYSIS

n_experiments = 100
window_sizes = [50, 100, 200]
M_values = [50, 100, 200]
eps_values = [0.1, 0.2, 0.3]
h_values = [np.log(T) * 2, np.log(T) * 3, np.log(T) * 4]

opt_index_phase1 = int(clairvoyant(classes,bids,prices, margins,conversion_rate_phase1,env_array)[0][0])
opt_phase1 = normEarnings_phase1[opt_index_phase1][0]
optimal_bid_index_phase1 = clairvoyant(classes,bids,prices, margins,conversion_rate_phase1,env_array)[1][0]
optimal_bid_phase1 = bids[int(optimal_bid_index_phase1)]

opt_index_phase2 = int(clairvoyant(classes,bids,prices, margins,conversion_rate_phase2,env_array)[0][0])
opt_phase2 = normEarnings_phase2[opt_index_phase2][0]
optimal_bid_index_phase2 = clairvoyant(classes,bids,prices, margins,conversion_rate_phase2,env_array)[1][0]
optimal_bid_phase2 = bids[int(optimal_bid_index_phase2)]

opt_index_phase3 = int(clairvoyant(classes,bids,prices, margins,conversion_rate_phase3,env_array)[0][0])
opt_phase3 = normEarnings_phase3[opt_index_phase3][0]
optimal_bid_index_phase3 = clairvoyant(classes,bids,prices, margins,conversion_rate_phase3,env_array)[1][0]
optimal_bid_phase3 = bids[int(optimal_bid_index_phase3)]

opt_phase1 = opt_phase1 * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
opt_phase2 = opt_phase2 * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
opt_phase3 = opt_phase3 * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)
opt = np.ones([T])
opt[:int(T/3)] = opt[:int(T/3)] * opt_phase1
opt[int(T/3):2*int(T/3)] = opt[int(T/3):2*int(T/3)]* opt_phase2
opt[2*int(T/3):] = opt[2*int(T/3):] * opt_phase3



fig, axs = plt.subplots(2,2, figsize=(14,7))
colors  = ['tab:orange', 'tab:purple', 'tab:green']
for window_size in window_sizes:
    swucb_rewards_per_experiments = []
    for e in tqdm(range(n_experiments)):
        env_swucb = deepcopy(env_array[0])

        swucb_learner = SWUCB_Learner(n_arms = n_prices, window_size = window_size)

        for t in range(0, T):

            pulled_arm = swucb_learner.pull_arm()
            reward = env_swucb.round(pulled_arm)
            swucb_learner.update(pulled_arm, reward)

        swucb_rewards_per_experiments.append(swucb_learner.collected_rewards)


    swucb_rewards_per_experiments = np.array(swucb_rewards_per_experiments)


    swucb_rewards_per_experiments[:int(T/3)] = swucb_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
    swucb_rewards_per_experiments[int(T/3):2*int(T/3)] = swucb_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
    swucb_rewards_per_experiments[2*int(T/3):] = swucb_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)

    i = window_sizes.index(window_size)

    axs[0][0].plot(np.cumsum(np.mean(opt - swucb_rewards_per_experiments, axis = 0)), colors[i])

axs[0][0].set_xlabel("time")
axs[0][0].set_ylabel("Regret")

axs[0][0].legend(["Regret SWUCB(WS ="+str(window_size)+")" for window_size in window_sizes])
axs[0][0].set_title("Cumulative Regret of SWUCB with different window sizes")

for m in M_values:
    cusum_rewards_per_experiments = []
    for e in tqdm(range(n_experiments)):
        env_cusum = deepcopy(env_array[0])

        cusum_learner = CUSUM_UCB_Learner(n_arms = n_prices, M = m, eps = eps_values[0], h = h_values[0])

        for t in range(0, T):

            pulled_arm = cusum_learner.pull_arm()
            reward = env_cusum.round(pulled_arm)
            cusum_learner.update(pulled_arm, reward)

        cusum_rewards_per_experiments.append(cusum_learner.collected_rewards)

    cusum_rewards_per_experiments = np.array(cusum_rewards_per_experiments)

    cusum_rewards_per_experiments[:int(T/3)] = cusum_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
    cusum_rewards_per_experiments[int(T/3):2*int(T/3)] = cusum_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
    cusum_rewards_per_experiments[2*int(T/3):] = cusum_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)

    i = M_values.index(m)

    axs[0][1].plot(np.cumsum(np.mean(opt - cusum_rewards_per_experiments, axis = 0)), colors[i])

axs[0][1].set_xlabel("time")
axs[0][1].set_ylabel("Regret")

axs[0][1].legend(["Regret CUSUM(M ="+str(m)+")" for m in M_values])
axs[0][1].set_title("Cumulative Regret of CUSUM with different M values")

for eps in eps_values:

    cusum_rewards_per_experiments = []
    for e in tqdm(range(n_experiments)):
        env_cusum = deepcopy(env_array[0])

        cusum_learner = CUSUM_UCB_Learner(n_arms = n_prices, M = M_values[1], eps = eps, h = h_values[0])
        for t in range(0, T):

            pulled_arm = cusum_learner.pull_arm()
            reward = env_cusum.round(pulled_arm)
            cusum_learner.update(pulled_arm, reward)

        cusum_rewards_per_experiments.append(cusum_learner.collected_rewards)

    cusum_rewards_per_experiments = np.array(cusum_rewards_per_experiments)

    cusum_rewards_per_experiments[:int(T/3)] = cusum_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
    cusum_rewards_per_experiments[int(T/3):2*int(T/3)] = cusum_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
    cusum_rewards_per_experiments[2*int(T/3):] = cusum_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)

    i = eps_values.index(eps)
    axs[1][0].plot(np.cumsum(np.mean(opt - cusum_rewards_per_experiments, axis = 0)), colors[i])

axs[1][0].set_xlabel("time")
axs[1][0].set_ylabel("Regret")

axs[1][0].legend(["Regret CUSUM(eps ="+str(eps)+")" for eps in eps_values])
axs[1][0].set_title("Cumulative Regret of CUSUM with different epsilon values")

for h in h_values:
    cusum_rewards_per_experiments = []
    for e in tqdm(range(n_experiments)):
        env_cusum = deepcopy(env_array[0])

        cusum_learner = CUSUM_UCB_Learner(n_arms = n_prices, M = M_values[1], eps = eps_values[0], h = h)
        for t in range(0, T):

            pulled_arm = cusum_learner.pull_arm()
            reward = env_cusum.round(pulled_arm)
            cusum_learner.update(pulled_arm, reward)

        cusum_rewards_per_experiments.append(cusum_learner.collected_rewards)

    cusum_rewards_per_experiments = np.array(cusum_rewards_per_experiments)

    cusum_rewards_per_experiments[:int(T/3)] = cusum_rewards_per_experiments[:int(T/3)] * env_array[0].n(optimal_bid_phase1) - env_array[0].cc(optimal_bid_phase1)
    cusum_rewards_per_experiments[int(T/3):2*int(T/3)] = cusum_rewards_per_experiments[int(T/3):2*int(T/3)] * env_array[0].n(optimal_bid_phase2) - env_array[0].cc(optimal_bid_phase2)
    cusum_rewards_per_experiments[2*int(T/3):] = cusum_rewards_per_experiments[2*int(T/3):] * env_array[0].n(optimal_bid_phase3) - env_array[0].cc(optimal_bid_phase3)

    i = h_values.index(h)
    axs[1][1].plot(np.cumsum(np.mean(opt - cusum_rewards_per_experiments, axis = 0)), colors[i])

axs[1][1].set_xlabel("time")
axs[1][1].set_ylabel("Regret")

axs[1][1].legend(["Regret CUSUM(h ="+str(h)+")" for h in h_values])
axs[1][1].set_title("Cumulative Regret of CUSUM with different h(threshold) values")

plt.show()
